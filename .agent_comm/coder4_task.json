{
  "agent_id": "coder4",
  "task_id": "task_2",
  "files": [
    {
      "name": "config.py",
      "purpose": "Agent and environment configuration",
      "priority": "high"
    },
    {
      "name": "requirements.txt",
      "purpose": "Python dependencies",
      "priority": "high"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.MA_2508.20818v1_cMALC_D_Contextual_Multi_Agent_LLM_Guided_Curricu",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.MA_2508.20818v1_cMALC-D-Contextual-Multi-Agent-LLM-Guided-Curricu with content analysis. Detected project type: agent (confidence score: 15 matches).",
    "key_algorithms": [
      "Robot",
      "Three",
      "Prompt",
      "Auxiliary",
      "Learning",
      "Making",
      "180000",
      "Paced",
      "Machine",
      "Arterial"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.MA_2508.20818v1_cMALC-D-Contextual-Multi-Agent-LLM-Guided-Curricu.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\ncMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with\nDiversity-Based Context Blending\nAnirudh Satheesh1Keenan Powell1Hua Wei2\nAbstract\nMany multi-agent reinforcement learning\n(MARL) algorithms are trained in fixed simu-\nlation environments, making them brittle when\ndeployed in real-world scenarios with more\ncomplex and uncertain conditions. Contextual\nMARL (cMARL) addresses this by parameter-\nizing environments with context variables and\ntraining a context-agnostic policy that performs\nwell across all environment configurations.\nExisting cMARL methods attempt to use\ncurriculum learning to help train and evaluate\ncontext-agnostic policies, but they often rely on\nunreliable proxy signals, such as value estimates\nor generalized advantage estimates that are\nnoisy and unstable in multi-agent settings due to\ninter-agent dynamics and partial observability.\nTo address these issues, we propose Contextual\nMulti-Agent LLM-Guided Curriculum Learn-\ning with Diversity-Based Context Blending\n(cMALC-D), a framework that uses Large Lan-\nguage Models (LLMs) to generate semantically\nmeaningful curricula and provide a more robust\nevaluation signal. To prevent mode collapse and\nencourage exploration, we introduce a novel\ndiversity-based context blending mechanism that\ncreates new training scenarios by combining\nfeatures from prior contexts. Experiments\nin traffic signal control domains demonstrate\nthat cMALC-D significantly improves both\ngeneralization and sample efficiency compared\nto existing curriculum learning baselines. We\nprovide code at https://github.com/\nDaRL-LibSignal/cMALC-D .\n1Department of Computer Science, University of Mary-\nland, College Park, Maryland, USA2School of Com-\nputing and Augmented Intelligence, Arizona State Univer-\nsity, Tempe, Arizona, USA. Correspondence to: Anirudh\nSatheesh <anirudhs@terpmail.umd.edu>, Keenan Powell <kpow-\nell1@terpmail.umd.edu>.\nProceedings of the International Conference on Machine Learning\nMAS Workshop , Vancouver, Canada. PMLR 267, 2025. Copyright\n2025 by the author(s).1. Introduction\nMulti-Agent Reinforcement Learning (MARL) has shown\npromising results across diverse applications, including real-\ntime strategy games (Samvelyan et al., 2019; Kurach et al.,\n2020), supply chain management (Liu et al., 2022; Mousa\net al., 2024), navigation and pathfinding (Zhang et al., 2024;\nSkrynnik et al., 2024), and traffic signal control (Chu et al.,\n2019; Jiang et al., 2022; Satheesh & Powell, 2025). These\nsuccesses are largely attributed to the ability of MARL algo-\nrithms, such as Independent Proximal Policy Optimization\n(IPPO) (De Witt et al., 2020) and Multi-Agent Proximal\nPolicy Optimization (MAPPO) (Yu et al., 2022), to train\nagents capable of coordination and cooperation.\nDespite this progress, generalization remains a key chal-\nlenge. Most MARL algorithms are trained in simulation\nenvironments with fixed or limited variability, making them\nbrittle when deployed in real-world scenarios where con-\nditions are more complex and uncertain. External factors\nsuch as noise (Bukharin et al., 2023; He et al., 2023) and\ndynamic changes (Zhang et al., 2020) can degrade MARL\nperformance substantially. These issues are amplified in\nmulti-agent settings due to the combinatorial explosion of\nagent interactions, which can destabilize learned policies\nand exacerbate overfitting to training conditions. Prior work\nhas addressed these challenges through meta-learning (Finn\net al., 2017; Mao et al., 2023; Harris et al., 2022; Zhang et al.,\n2021), invariant representation learning (Chen & Zhang,\n2024; McClellan et al., 2024; 2025), and domain adapta-\ntion (Tzeng et al., 2020; Da et al., 2024). While effective,\nthese approaches often rely on predefined environment dis-\ntributions and cannot guarantee robustness to unseen or\nout-of-distribution scenarios.\nTo address the challenge of poor generalization to unseen\nor out-of-distribution environments, we build on the con-\ntextual MARL (cMARL) framework (Jayawardana et al.,\n2024), which explicitly represents environment variability\nthrough a context variable c(Hallak et al., 2015). General-\nization in cMARL is commonly improved via curriculum\nlearning (Bengio et al., 2009), where agents are trained on\ncontexts that gradually increase in difficulty or novelty (To-\nbin et al., 2017; Klink et al., 2020; Jiang et al., 2021; Eimer\net al., 2021; Parker-Holder et al., 2023). This allows agents\n1arXiv:2508.20818v1  [cs.LG]  28 Aug 2025\n\n--- Page 2 ---\ncMALC-D\nto acquire transferable skills incrementally and improves\nrobustness at test time.\nWhile curriculum learning improves generalization in con-\ntextual multi-agent reinforcement learning (MARL) by or-\ndering training environment contexts by difficulty or novelty,\nexisting approaches often depend on handcrafted heuristics\nor static curriculum schedules. These strategies may strug-\ngle to adapt to the evolving agents or the complex dependen-\ncies among context variables in dynamic environments. To\naddress these limitations, we explore the use of Large Lan-\nguage Models (LLMs) as high-level curriculum designers.\nRecent advancements in LLMs, such as GPT-4o, Qwen, and\nGemini, have shown strong capabilities in reasoning, plan-\nning, and abstraction (Song et al., 2023; Yao et al., 2022;\nXu et al., 2023; Ma et al., 2023; Dong et al., 2024). These\nmodels can operate in diverse domains through in-context\nlearning, suggesting their potential for adaptively generating\nand sequencing environment contexts based on the agent\u2019s\ncurrent state and performance.\nIn this work, we propose Contextual Multi-Agent LLM-\nGuided Curriculum Learning with Diversity-Based Context\nBlending (cMALC-D), a novel framework that integrates\nLLMs into contextual MARL to dynamically generate train-\ning curricula. Specifically, in cMALC-D, the LLM acts as\na high-level controller that observes the agent\u2019s learning\nprogress and adaptively proposes new environment contexts\nby reasoning over the space of context variables. To enhance\ncoverage and prevent overfitting to narrow task distributions,\nwe introduce a diversity-based context blending mechanism\nthat mixes previously sampled contexts to construct novel\nyet meaningful training conditions. This LLM-guided pro-\ncess allows the curriculum to evolve in tandem with agent\ncapabilities, providing more targeted and generalizable train-\ning experiences. We evaluate cMALC-D in multiple traffic\ncontrol scenarios, where environments are naturally high-\ndimensional and dynamic. Results show that our approach\nsignificantly improves generalization to unseen environment\ncontexts with higher sample efficiency compared to existing\nself-paced or handcrafted curriculum strategies.\nOur contributions are outlined as follows:\n\u2022We introduce Contextual Multi-Agent LLM-Guided\nCurriculum Learning with Diversity-Based Context\nBlending (cMALC-D), a framework that leverages\nLarge Language Models (LLMs) to generate semanti-\ncally meaningful context-based curricula for training\nMARL agents, improving generalization to unseen en-\nvironment configurations.\n\u2022Experiments in multiple traffic-based environments\ndemonstrate that our approach achieves better gener-\nalization compared to other self-paced curricula, with\nhigher sample efficiency.2. Related Work\n2.1. Generalization in MARL\nMeta Learning One approach to improving MARL gen-\neralizability has been to apply meta-learning to MARL.\nMeta learning develops a good policy initialization that can\nrapidly adapt to new tasks or environmental configurations\nwith minimal additional training. This can be done with\nModel-Agnostic Meta-Learning (MAML) algorithms (Finn\net al., 2017; Mao et al., 2023) or through context-based la-\ntent variables (Wen et al., 2024; Zhang et al., 2021), both of\nwhich train over a task distribution to develop transferable\ninductive biases. However, such methods face significant\nscalability issues. MAML requires expensive Hessian com-\nputations on the order of the number of model parameters or\nfirst-order approximations (Fallah et al., 2021; Nichol et al.,\n2018). Additionally, prior work focuses on multi-agent solu-\ntion concepts for two-player games in tabular settings (Mao\net al., 2023; Harris et al., 2022; Zhang et al., 2022), which\nare not extendable to many agents parameterized by neural\nnetworks.\nEquivariance Multi-agent environments exhibit symme-\ntries that can be leveraged to improve generalization using\nequivariant networks (McClellan et al., 2024; 2025; Chen\n& Zhang, 2024). To fully exploit such invariances, each\nagent must be parameterized with architectures that respect\nthese symmetries, such as EGNNs (Satorras et al., 2021),\nE3-MPNNs (Brandstetter et al., 2021), or E2GN2s (McClel-\nlan et al., 2024). However, these methods typically require\naccess to the full global state, limiting their applicability in\npartially observable settings, which encompass most multi-\nagent environments.\nSim-to-Real It is often very challenging to train MARL\nagents on real-world environment data due to limited acces-\nsibility, high costs, and the intricate complexities of real-\nworld systems. A common workaround is to train agents\nin simulated environments instead. However, simulations\noften fail to capture the full range of noise and unpredictabil-\nity found in the real world, making it difficult for agents to\ntransfer their skills effectively (Dulac-Arnold et al., 2021).\nSeveral techniques have been proposed to address this sim-\nto-real gap, one notable method being Domain Random-\nization. Domain Randomization exposes agents to a wide\nrange of settings by randomizing key environment parame-\nters (Zhao et al., 2020). While effective in some cases, it can\nproduce unrealistic environments that fail to teach agents\ntransferable skills, an issue that becomes more pronounced\nin MARL settings. Additional methods try to minimize the\nsim-to-real gap by aligning simulation trajectories to real\nones (Tzeng et al., 2020), or by developing an inverse model\nto correct for bias in the simulation environment (Da et al.,\n2024), but these methods require information from specific\nreal-world environments, which may not be available. Re-\n2\n\n--- Page 3 ---\ncMALC-D\ncently, large language model (LLM)-based methods have\nshown promise in generating more coherent and purposeful\nenvironments, but their application has been largely limited\nto single-agent contexts (Ma et al., 2024; Zala et al., 2024).\n2.2. Self-Paced Curriculum Learning\nIn contrast to prior approaches such as meta-learning, equiv-\nariant architectures, and sim-to-real, curriculum learning\n(Bengio et al., 2009) aims to improve generalization by\nstructuring agents\u2019 learning process, progressing from eas-\nier to more challenging tasks. Additional methods explore\nautomated curriculum generation to avoid manual design.\nFor example, Sukhbaatar et al. (2017) leverages self-play\nto minimize the number of training episodes by generat-\ning progressively harder tasks through agent interactions.\nDendorfer et al. (2020) and Florensa et al. (2018) use Gen-\nerative Adversarial Networks (GANs) to create challenging\ngoals tailored to the agent\u2019s capabilities. Portelas et al.\n(2020) uses a Gaussian Mixture Model (GMM) to model\nthe task space and align a student\u2019s learning trajectory with\na teacher-generated curriculum. However, each of these\nrequires an auxiliary model to determine the learnability\nof a task. Instead, Klink et al. (2020); Eimer et al. (2021)\nuse self-paced learning, which uses agents\u2019 performance to\norder the curriculum. Thus, each task is tailored to each\nagent\u2019s abilities and ensures that the learning progress is\nmore self-contained.\n3. Contextual Multi-Agent Reinforcement\nLearning\nWe formulate cMALC-D as a contextual decentralized par-\ntially observable Markov decision process (cDec-POMDP).\nA cDec-POMDP is parameterized by the tuple Mc=\n(N,S,A,Tc, R,\u2126,O, \u03b3, \u00b5), where Nis the set of agent\nindices, denoting a system of n=|N|cooperative agents.\nSis the joint state space shared across all agents, and\nA=Q\ni\u2208NAiis the joint action space, where Aiis the ac-\ntion space for agent i. The transition function Tc:S \u00d7A \u2192\n\u2206(S)determines the next state distribution given the cur-\nrent state and joint action under context c, while the reward\nfunction R:S \u00d7 A \u2192 Rmaps state-action pairs to a scalar\nreward. \u2126 =Q\ni\u2208N\u2126idenotes the joint observation func-\ntion, where \u2126i:S \u2192 Oiprovides a private observation to\nagent i, andO=Q\ni\u2208NOiis the joint observation space.\nThe discount factor \u03b3\u2208[0,1)specifies the importance of\nfuture rewards, and \u00b5:S \u2192 [0,1]is the initial state distri-\nbution.\nTo model task variation, we define a distribution over con-\ntexts c\u2208 C, where each context specifies a different task\ninstance by altering the transition functions. This induces\na setMC={Mc|c\u2208 C} of decentralized POMDPs, each\ncorresponding to a distinct environment. Each Mcencodesa different task instantiation with a different transition func-\ntion, and we assume that the reward function and the state,\naction, and observation spaces remain fixed across all con-\ntexts.\nThe objective of a policy \u03c0in a cDec-POMDP is to maxi-\nmize the expected return over the context distribution and\nover finite horizon H:\n\u03c0\u2217= arg max\n\u03c0\u2208\u03a0Ec\u223cC\"\nE\u03c0\"H\u22121X\nt=0\u03b3tR(st, at)\f\f\f\f\u03c0, c##\nwhere \u03a0 ={\u03c0= (\u03c01, . . . , \u03c0n)|\u03c0i:oi\nt\u2192ai\nt}is the set\nof decentralized policies, with each agent i\u2208 N selecting\nactions based only on its local observation oi\nt. The goal is to\nlearn a context-agnostic policy that generalizes well across\nthe set of Dec-POMDPs MC.\n4. Evolutionary LLM Self-Paced Curriculum\nIn this section, we outline the motivations of CMALC-D\nand its implementation.\n4.1. Limitations of Existing Self-Paced Curriculum\nLearning Algorithms\nThere are two major limitations of current self-paced curricu-\nlum learning algorithms for contextual MARL (cMARL),\nboth of which hinder efficient and robust generalization:\n\u2022Random Task Sampling Most existing approaches\ngenerate new contexts by randomly sampling from the\ncontext space, without considering semantic relation-\nships or difficulty progression between sampled envi-\nronments. This can lead to large variations between\ncontexts across training episodes, making learning un-\nstable and inefficient. For example, an agent might\nface an easy environment followed by a drastically\nmore difficult or qualitatively different one, forcing it\nto relearn strategies rather than incrementally building\noff prior knowledge.\n\u2022Unreliable Proxy Evaluation Current methods typi-\ncally rely on policy metrics like the value estimate or\nthe Generalized Advantage Estimate (GAE) to evaluate\nagent performance and determine subsequent contexts\nto train on. While these methods work well for single-\nagent tasks with many environment updates (Jiang\net al., 2021; Parker-Holder et al., 2023), these met-\nrics can be unreliable during early training phases and\nunder sudden domain transfer. They can also be over-\nconfident in underexplored regions of the context space\nand may not provide a meaningful indication of how\nwell agents will generalize to unseen contexts.\nThese challenges underscore the need for more structured,\n3\n\n--- Page 4 ---\ncMALC-D\nsemantically aware curriculum generation strategies cou-\npled with more robust evaluation signals that better reflect\ngeneralization and learning progress across context varia-\ntions.\n4.2.Evolutionary LLM-Guided Curriculum for Context\nGeneration\nTo address these limitations, we propose cMALC-D, a novel\ncurriculum learning strategy for contextual MARL that com-\nbines the structured reasoning capabilities of large language\nmodels (LLMs) with an exploration mechanism based on\ntask arithmetic. This approach improves both the generation\nof semantically meaningful environment contexts and the\nrobustness of policy evaluation under limited feedback.\nLLM-Guided Context Generation Instead of randomly\nsampling from a context space C, we leverage a large lan-\nguage model to reason over a sliding window of past training\nresults and generate new contexts that reflect a meaningful\nprogression in difficulty or diversity. At each curriculum\nstep, the LLM receives a window of the most recent contexts\n{ct\u2212w,\u00b7\u00b7\u00b7, ct}and their associated performance metrics\n{mt\u2212w,\u00b7\u00b7\u00b7mt}when trained on MARL algorithm A. It\nthen leverages this history to propose a new context that ei-\nther incrementally challenges the current multi-agent policy\nor targets known weaknesses observed in recent episodes.\nDiversity-Based Context Blending To avoid curriculum\nstagnation and encourage exploration of the context space,\nwe monitor the similarity between successive contexts. If\nthe LLM repeatedly generates highly similar contexts, indi-\ncating potential mode collapse in curriculum progression,\nwe enable a diversity mechanism. Specifically, when the\nnumber of consecutive similar tasks exceeds a threshold, we\nblend the current LLM-proposed context with a randomly\nsampled context from history. This interpolation helps inject\nnovelty in the curriculum while avoiding sudden changes in\nthe curriculum.\nAlternating Policy Training and Context Generation\nSimilar to (Ma et al., 2023), we alternate between policy\ntraining and context generation. After each training phase,\nthe agent\u2019s performance on the current context is recorded\nand passed to the LLM, which conditions on a sliding win-\ndow of past evaluations to generate the next context. This\napproach\u2014in-context context generation\u2014enables the LLM\nto implicitly reason about task difficulty and progression\nwithout gradient updates or handcrafted reward shaping.\nWe present the full algorithm in Algorithm 1.\n5. Experimental Setup\nIn this section, we show experimental details and the base-\nlines we evaluate cMALC-D against.Algorithm 1 Contextual Multi-Agent LLM-Guided Cur-\nriculum Learning with Diversity Based Context Blending\n(cMALC-D)\nRequire: MARL algorithm A, context space C, LLM M,\nblending factor \u03b1, sliding window size w, similarity\nthreshold \u03b4, max similar count k, initial context c0\n1:Initialize context buffer H\u2190[], similarity counter\ns\u21900\n2:Set current context c0\n3:forcurriculum step t= 0,1, . . . , T do\n4: Train policy \u03c0ton context ct, collect performance\nmetric mt\n5: Append (ct, mt)by algorithm Ato context buffer H\n6: Construct sliding window Hw =\n{(ct\u2212w, mt\u2212w), . . . , (ct, mt)}\n7: Query MwithHwto generate new context cM\nt+1\n8: Compute similarity \u03c3\u2190Sim({ct\u2212w,\u00b7\u00b7\u00b7, ct}, cM\nt+1)\n9: if\u03c3\u2265\u03b4then\n10: Increment similarity counter s\u2190s+ 1\n11: else\n12: Reset similarity counter s\u21900\n13: end if\n14: ifs\u2265kthen\n15: Sample random prior context cr\u223cUniform (H)\n16: Blend: ct+1\u2190\u03b1cr+ (1\u2212\u03b1)cM\nt+1\n17: Reset similarity counter s\u21900\n18: else\n19: Setct+1\u2190cM\nt+1\n20: end if\n21:end for\n5.1. Experimental Details\nWe evaluate cMALC-D on three autonomous traffic signal\ncontrol environments based on real-world data. We choose\nto evaluate on these datasets for three reasons:\n\u2022Real-World Relevance Autonomous traffic signal con-\ntrol has critical real-life applications in reducing con-\ngestion, emissions, and travel time. This also gives\nseveral metrics outside of reward that we can use for\nevaluation. More information about environment met-\nrics can be found in Appendix A.2\n\u2022Well Defined Context Space Each vehicle in the traffic\nsignal control environment is clearly defined by several\nvariables that can be altered to generate new contexts.\nMore details about context parameterization can be\nfound in Appendix A.3.\n\u2022Multi-Agent Coordination Challenges Traffic sig-\nnal control inherently involves decentralized decision-\nmaking and coordination among agents, providing a\nnatural testbed for evaluating the scalability and gener-\n4\n\n--- Page 5 ---\ncMALC-D\nalization of our cMARL curriculum framework.\nWe run our experiments with the CityFlow environment,\nwhich is a realistic and efficient traffic flow simulator writ-\nten in C++ (Zhang et al., 2019). It is also compatible with\nMARL algorithms via integration with the Gymnasium Li-\nbrary (Brockman, 2016). We train all policies with MAPPO\n(Yu et al., 2022), but any MARL algorithm will work; we\nchoose to use MAPPO due to its efficiency compared to\noff-policy algorithms.\nWe use the Qwen2.5-7B-Instruct model (Team, 2024) as a\nhigh-level curriculum designer. To increase computational\nefficiency, we use the vLLM package (Kwon et al., 2023) to\nstreamline inference and leverage activation-aware weight\nquantization (Lin et al., 2024) to reduce memory usage.\nThus, we only require 2 RTX 2080 TI GPUs for LLM-\nbased experiments, and 1 RTX 2080 TI GPU for non-LLM\nexperiments, and there is a neglible difference in MARL\npolicy training time.\nFor all experiments, we alternate between expanding the\ncurriculum and training the MARL policy for 500 episodes,\nwhere each episode is 360 timesteps, resulting in 180,000\ntrajectories per training phase. This is done via the Central-\nized Training Decentralized Execution (CTDE) paradigm,\nwhere training is done with access to the global joint obser-\nvation, but agents only have access to their local observa-\ntions during policy execution. We reserve a held-out test\nset of 5 contexts and evaluate the current policy every 5\nepisodes using greedy action sampling. After training, we\ngenerate 10 additional random contexts to assess general-\nization performance in both a zero-shot setting and after a\nbrief finetuning phase of 5 episodes. In practice, we observe\nminimal differences between the zero-shot and finetuned\npolicies; therefore, we report performance metrics, such as\ndelay time, throughput, and wait time, based on the fine-\ntuned policy. Additional details about these metrics can be\nfound in Appendix A. All experiments are repeated across\n5 random seeds to ensure statistical robustness.\n5.2. Baselines\nIn this work, we compare cMALC-D to 5 other curriculum\nlearning algorithms.\n\u2022No Curriculum : For this algorithm, we simply train\nMARL agents using MAPPO on a single training envi-\nronment for all epochs, as would be done normally.\n\u2022Domain Randomization (Tzeng et al., 2020): Domain\nRandomization randomly generates an environment\nusing a set of parameters and a probability distribution\nfor each parameter at each timestep. Then, a policy\nis trained on a rollout from that environment at each\ntimestep.\u2022Prioritized Level Replay (PLR) (Jiang et al., 2021):\nPLR is a curriculum learning algorithm that keeps track\nof previously generated environments (or levels) and\nscores them based on learning potential using the Tem-\nporal Difference Error. After evaluating each level,\nit decides randomly whether to replay the previously\nplayed level with the highest learning potential or cre-\nate a new level.\n\u2022Adversarially Compounding Complexity by Editing\nLevels (ACCEL) (Parker-Holder et al., 2023): ACCEL\nis similar to PLR, except instead of training directly\non previous levels with high TD-error, ACCEL main-\ntains a population of levels with high TD-error and\nmakes small and randomized mutations to those levels,\nand decides randomly whether to play on one of the\npopulation of those levels or generate a new one.\n\u2022Self-Paced Context Evaluation (SPACE) (Eimer\net al., 2021): SPACE uses the value estimate to build a\ncurriculum. Taking V\u03c0\nt(s0, ci)to be the estimated total\nexpected reward by following policy \u03c0from starting\nstates0with context ciafterttraining steps, SPACE\ntakes V\u03c0\nt(s0, ci)\u2212V\u03c0\nt\u22121(s0, ci)to be the performance\nimprovement capacity (PIC) for ci. SPACE trains con-\ntinually in environments with high PIC until it con-\nverges, sampling a new environment from the context\nspace.\n6. Results\nIn this section, we describe the results of cMALC-D com-\npared to the baselines and perform ablation studies to un-\nderstand individual components of the algorithm. We aim\nto answer three main questions: What is the generalization\nperformance of the algorithm? How does the diversity mech-\nanism influence context generation? What kinds of contexts\nare generated by the LLM curriculum?\n6.1. Generalization Performance\nWe show the generalization performance of cMALC-D\nagainst the baseline algorithms in Tables 1a, 1b, and 1c.\nAcross all three environments, JN 1\u00d73, HZ, and JN 3\u00d74,\ncMALC-D consistently outperforms or matches all other\ncurriculum strategies on the test reward and specific traffic\npolicy metrics, such as average delay and throughput. For\nexample, in JN 1\u00d73, it achieves the highest test reward\n(29.01\u00b10.32) and throughput ( 3073.22\u00b1114.06) while\nreducing wait time by 2%over the second-best algorithm.\nSimilar trends hold for the HZ and JN 3\u00d74environments.\nStructured curricula are necessary to learn generaliz-\nable policies. In contrast, Domain Randomization under-\nperforms compared to cMALC-D, often giving 3rd or 4th\nplace results across performance metrics (e.g., 4th place in\n5\n\n--- Page 6 ---\ncMALC-D\naverage delay in HZ with 241.79\u00b135.60vs. cMALC-D\u2019s\n146.96\u00b119.93). While it occasionally yields high through-\nput or test rewards over other algorithms (e.g., 2nd place test\nreward of 27.55\u00b10.41in JN 1\u00d73), these gains are unreli-\nable and highly environment-dependent. This inconsistency\nhighlights a fundamental limitation of randomization-based\nstrategies: while they expose agents to a wide range of en-\nvironments, they do so without considering progression or\ncontext relevance. As a result, agents may struggle to learn\nthe high-level coordination skills necessary for generaliza-\ntion due to rapid context switching in the curriculum.\nOriginal context can be a useful prior, but may en-\ncourage overfitting. Training without a curriculum can\nyield strong performance, particularly in the JN environ-\nments, where No Curriculum frequently ranks second af-\nter cMALC-D (e.g., throughput of 3704.17\u00b1147.39vs.\ncMALC-D\u2019s 3795.46\u00b1159.50in JN 3\u00d74). This suggests\nthat the original context provides a good prior, enabling\nagents to learn basic coordination strategies. However, its\neffectiveness diminishes in more diverse settings (most no-\ntably in the HZ environment, where its average delay of\n339.09\u00b153.28is worse than cMALC-D\u2019s 146.96\u00b119.93),\nwhere it performs significantly worse than cMALC-D and\nexhibits high variance even in the JN environments (e.g.,\ntest reward standard deviation of 2.44vs.1.53in JN 3\u00d74).\nThis drop indicates that without curriculum learning, agents\nmay overfit to features in the original context, which limits\ngeneralizability.\nLLM-based context evaluation provides a stable signal\nfor effective curriculum learning. While some methods\nlike ACCEL and PLR incorporate similar automatic curricu-\nlum schemes, they rely heavily on policy evaluation signals,\nsuch as value functions or generalized advantage estimates,\nto select and schedule tasks. While these signals can be\nhighly effective in single-agent domains with millions of\nenvironment updates, they can be noisy or unreliable in\nMARL due to non-stationarity, partial observability, and\ninter-agent dependencies (e.g., ACCEL\u2019s inconsistent rank-\nings from 4th place in JN 1\u00d73to 2nd place in HZ). On the\nother hand, cMALC-D\u2019s context selection strategy promotes\ngradual skill acquisition that transfers well across diverse\ncontexts. This is due to using language-based evaluations\nthat can capture qualitative improvements that traditional\nmetrics might overlook (demonstrated by cMALC-D\u2019s top\nperformance across all environments with test rewards of\n29.01\u00b10.32,172.87\u00b11.03, and116.57\u00b11.53in JN 1\u00d73,\nHZ, and JN 3\u00d74, respectively).\n6.2. Influence of the Diversity Mechanism\nTo evaluate the impact of the diversity mechanism, we com-\npare three variants of our method: the full version with\nsimilarity-based diversity ( cMALC-D ), a baseline withoutthe diversity mechanism ( cMALC ), and a variant that ap-\nplies task arithmetic with random probability \u03f5= 0.1in-\nstead of using similarity checks ( cMALC- \u03f5).\nFigure 1 shows the mean test return across five held-out test\ntasks (rows) and three datasets (columns), with each curve\naveraged over five random seeds. On the easier Jinan (1\u00d73)\ndataset (left column), cMALC-D yields clear performance\nimprovements. This is especially evident in test tasks 1,\n3, and 4 (first, third, and fourth rows), where cMALC-D\nconsistently achieves a test return around 31, outperforming\nthe other variants by approximately two points on average.\nOn the Hangzhou dataset (middle column), performance\nacross all three variants is more similar, but cMALC-D\nstill shows consistent gains. Across all settings, the inclu-\nsion of the diversity mechanism improves sample efficiency\nand accelerates convergence. However, cMALC- \u03f5exhibits\ngreater instability. In particular, test tasks 1 through 4 (first\nfour rows) show that cMALC- \u03f5often suffers sharp drops in\nperformance around 170,000 timesteps, sometimes losing\nmuch of the progress previously made.\nPerformance is uniformly lower on the most challenging\nJinan (3\u00d74)dataset (right column). We attribute this to\nthe increased traffic density, as small changes in the context\ncan cause substantial environment changes. Nevertheless,\ncMALC-D remains the top-performing variant. Notably, the\nbaseline cMALC frequently shows a decline in test reward\nover time (particularly visible in test tasks 1-4), which sug-\ngests mode collapse and poor generalization to test contexts,\nfurther highlighting the need for context diversity during\ntraining.\n6.3. What kinds of contexts are generated?\nTo better understand how curricula evolve, we visualize\ncontext trajectories using heatmaps in Figures 3, 4, and\n5. Across all three environments, we observe that context\nfeatures tend to evolve gradually over time rather than un-\ndergoing abrupt shifts. This gradual progression facilitates\na smoother learning process for the agents and supports our\nfinding that cMALC-D outperforms Domain Randomiza-\ntion due to a more controlled approach of generating diverse\ncontexts.\nWe also observe that in the curriculum for the JN 1\u00d73\nenvironment, multiple context features are frequently up-\ndated simultaneously. For instance, around episode 125,\nchanges occur simultaneously in maxSpeed andminGap ,\nalong with adjustments to vehicle length andwidth . A\nsimilar cluster of updates appears around episode 375, in-\nvolving both maxNegAcc andusualNegAcc , and in the\nHZ environment near episode 450.\nThis raises an interesting question: do LLMs internally form\nor leverage semantic relationships between features, such as\n6\n\n--- Page 7 ---\ncMALC-D\nTable 1. Performance metrics across all environments. Best results per metric are shown in bold and second-best results are underlined .\nWe include uncertainty within one standard deviation of the mean, averaged over 5 seeds.\nCurriculum Average Time Throughput Average Wait Time Average Delay Test Reward\nNo Curriculum 816.00 \u00b135.63 3032.72 \u00b1114.20 765.84 \u00b136.76 728.34 \u00b137.56 27.56\u00b10.46\nDomain Randomization 865.64\u00b143.08 2807 .22\u00b1141.63 816 .64\u00b144.41 774 .43\u00b146.66 27.55\u00b10.41\nPLR 841.74\u00b140.63 2955 .79\u00b1127.53 795 .98\u00b141.65 772 .72\u00b141.46 27 .61\u00b10.44\nACCEL 860.81\u00b139.57 2813 .86\u00b1130.81 819 .49\u00b140.75 806 .07\u00b141.06 26 .55\u00b10.45\nSPACE 939.39\u00b139.96 2544 .24\u00b1133.01 898 .71\u00b141.15 875 .71\u00b141.19 26 .89\u00b10.43\ncMALC-D 809.39 \u00b136.37 3073.22 \u00b1114.06 750.81 \u00b136.99 718.77 \u00b137.26 29.01 \u00b10.32\n(a) JN 1\u00d73Performance Metrics\nCurriculum Average Time Throughput Average Wait Time Average Delay Test Reward\nNo Curriculum 710.01\u00b142.78 2188 .94\u00b174.78 342 .84\u00b151.55 339 .09\u00b153.28 164 .04\u00b12.68\nDomain Randomization 637.52\u00b131.61 2318 .41\u00b153.49 239 .36\u00b132.60 241 .79\u00b135.60 168 .53\u00b11.44\nPLR 611.26\u00b126.48 2390 .56\u00b138.11 199 .22\u00b123.74 175 .96\u00b122.20 171 .85\u00b11.02\nACCEL 615.02 \u00b125.46 2393.30\u00b135.37 212 .28\u00b121.12 178 .54\u00b119.56 171 .07\u00b11.17\nSPACE 588.84\u00b125.11 2440.18 \u00b132.02 166.75 \u00b118.13 140.25 \u00b116.30 172.90 \u00b11.05\ncMALC-D 586.86 \u00b125.50 2440.09 \u00b135.45 163.92 \u00b119.71 146.96 \u00b119.93 172.87 \u00b11.03\n(b) HZ Performance Metrics\nCurriculum Average Time Throughput Average Wait Time Average Delay Test Reward\nNo Curriculum 829.92 \u00b136.98 3704.17 \u00b1147.39 566.12 \u00b143.18 550.47 \u00b146.03 115.77 \u00b12.44\nDomain Randomization 976.73\u00b137.67 3108 .67\u00b1151.36 753 .96\u00b145.58 756 .78\u00b149.21 112 .38\u00b11.52\nPLR 992.79\u00b147.30 3071 .38\u00b1188.12 781 .35\u00b157.70 786 .23\u00b161.33 111 .98\u00b11.74\nACCEL 1077.27\u00b142.13 2699 .78\u00b1166.09 903 .94\u00b150.26 921 .75\u00b152.73 110 .28\u00b11.60\nSPACE 899.94\u00b143.34 3447 .63\u00b1172.27 658 .07\u00b151.30 638 .21\u00b155.47 114 .47\u00b11.64\ncMALC-D 815.81 \u00b139.51 3795.46 \u00b1159.50 557.96 \u00b146.57 529.83 \u00b150.28 116.57 \u00b11.53\n(c) JN 3\u00d74Performance Metrics\ncoupling speed with spacing, or braking with acceleration,\nto propose more coherent or pedagogically aligned context\ntransitions? To answer this question, we construct correla-\ntion matrices between features in the cMALC-D generated\ncurriculum in Figures 6, 7, 8.\nIn the JN 1\u00d73environment, which contains fewer vehi-\ncles and simpler dynamics, the LLM identifies and exploits\nmeaningful semantic patterns. For example, maxNegAcc\n(maximum negative acceleration or braking) has a weak\ncorrelation with other features, suggesting that the model\nimplicitly recognizes the limited need for braking behaviors\ndue to the sparsity of interactions in a small-scale scenario.\nIn the HZ environment, vehicle width shows a similarly low\ncorrelation, despite the model not being given lane dimen-\nsions. This suggests the LLM infers structural properties of\nthe environment, like tight lane-to-vehicle width ratios, and\ndevelops curricula that do not depend on this feature, purely\nfrom context patterns. Such reasoning allows it to generate\nmore coherent and pedagogically effective curricula without\nexplicit environmental parameters.On the other hand, the LLM also captures strong correla-\ntions between semantically coupled features. For example,\nin the JN 3\u00d74environment (Figure 6), the correlation\nbetween maxSpeed andminGap is 0.71, reflecting the\nLLM\u2019s implicit understanding that higher speeds require\nlarger spacing between vehicles to ensure safety. Similarly,\nin the HZ environment (Figure 7), usualPosAcc and\nmaxPosAcc are correlated with a value of 0.83, indicating\nthe LLM\u2019s awareness that general acceleration behavior is\noften constrained by maximum performance limits.\n7. Conclusion and Future Work\nIn this paper, we develop cMALC-D, an LLM-based curricu-\nlum learning algorithm for contextual MARL. Our method\nleverages the reasoning capabilities of LLMs to generate se-\nmantically meaningful curricula. We also introduce a novel\ndiversity-based mechanism based on task arithmetic from\ncontinual learning to encourage exploration in the context\nspace and avoid mode collapse. Our experiments on three\nreal-world traffic environments show that cMALC-D en-\n7\n\n--- Page 8 ---\ncMALC-D\n(a) Jinan (1\u00d73), Task 1\n (b) Hangzhou, Task 1\n (c) Jinan (3\u00d74), Task 1\n(d) Jinan (1\u00d73), Task 2\n (e) Hangzhou, Task 2\n (f) Jinan (3\u00d74), Task 2\n(g) Jinan (1\u00d73), Task 3\n (h) Hangzhou, Task 3\n (i) Jinan (3\u00d74), Task 3\n(j) Jinan (1\u00d73), Task 4\n (k) Hangzhou, Task 4\n (l) Jinan (3\u00d74), Task 4\n(m) Jinan (1\u00d73), Task 5\n (n) Hangzhou, Task 5\n (o) Jinan (3\u00d74), Task 5\nFigure 1. Test return over training timesteps across three datasets (columns) and five held-out test tasks (rows). Each plot shows the\nperformance of three algorithm variants (cMALC-D, cMALC, and cMALC- \u03f5) averaged over five random seeds. The left column shows\nresults for the Jinan (1 \u00d73) dataset, the middle column for Hangzhou, and the right column for the more challenging Jinan (3 \u00d74) dataset.\nhances MARL policy generalization and sample efficiency\nover a variety of environment configurations.\nWhile our formulation is based on modifying the transition\nfunction by altering context features, there are other methods\nto model different environment configurations. For example,\nin the traffic signal control setting, sensor malfunctions can\ncause agents to return noisy data (Yang et al., 2024). One\npossible way to include this is to have the LLM generate amore general context, such as a noisy environment configu-\nration parameterized by a probability distribution. Another\navenue for future work is explicitly encoding semantic fea-\nture relationships to augment self-paced curricula.\nReferences\nBengio, Y ., Louradour, J., Collobert, R., and Weston, J.\nCurriculum learning. In Proceedings of the 26th Annual\n8\n\n--- Page 9 ---\ncMALC-D\nInternational Conference on Machine Learning , ICML\n\u201909, pp. 41\u201348, New York, NY , USA, 2009. Associa-\ntion for Computing Machinery. ISBN 9781605585161.\ndoi: 10.1145/1553374.1553380. URL https://doi.\norg/10.1145/1553374.1553380 .\nBrandstetter, J., Hesselink, R., van der Pol, E., Bekkers,\nE. J., and Welling, M. Geometric and physical quantities\nimprove e (3) equivariant message passing. arXiv preprint\narXiv:2110.02905 , 2021.\nBrockman, G. Openai gym. arXiv preprint\narXiv:1606.01540 , 2016.\nBukharin, A., Li, Y ., Yu, Y ., Zhang, Q., Chen, Z., Zuo,\nS., Zhang, C., Zhang, S., and Zhao, T. Robust multi-\nagent reinforcement learning via adversarial regulariza-\ntion: Theoretical foundation and stable algorithms. Ad-\nvances in Neural Information Processing Systems , 36:\n68121\u201368133, 2023.\nChen, D. and Zhang, Q. e(3)-equivariant actor-critic\nmethods for cooperative multi-agent reinforcement learn-\ning, 2024. URL https://arxiv.org/abs/2308.\n11842 .\nChu, T., Wang, J., Codec\u00e0, L., and Li, Z. Multi-agent\ndeep reinforcement learning for large-scale traffic signal\ncontrol. IEEE transactions on intelligent transportation\nsystems , 21(3):1086\u20131095, 2019.\nDa, L., Gao, M., Mei, H., and Wei, H. Prompt to transfer:\nSim-to-real transfer for traffic signal control with prompt\nlearning. In Proceedings of the AAAI Conference on\nArtificial Intelligence , volume 38, pp. 82\u201390, 2024.\nDe Witt, C. S., Gupta, T., Makoviichuk, D., Makoviychuk,\nV ., Torr, P. H., Sun, M., and Whiteson, S. Is indepen-\ndent learning all you need in the starcraft multi-agent\nchallenge? arXiv preprint arXiv:2011.09533 , 2020.\nDendorfer, P., Osep, A., and Leal-Taix\u00e9, L. Goal-gan: Mul-\ntimodal trajectory prediction based on goal position es-\ntimation. In Proceedings of the Asian Conference on\nComputer Vision , 2020.\nDong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia,\nH., Xu, J., Wu, Z., Liu, T., Chang, B., Sun, X., Li, L.,\nand Sui, Z. A survey on in-context learning, 2024. URL\nhttps://arxiv.org/abs/2301.00234 .\nDulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J.,\nPaduraru, C., Gowal, S., and Hester, T. Challenges of\nreal-world reinforcement learning: definitions, bench-\nmarks and analysis. Machine Learning , 110(9):2419\u2013\n2468, Sep 2021. ISSN 1573-0565. doi: 10.1007/\ns10994-021-05961-4. URL https://doi.org/10.\n1007/s10994-021-05961-4 .Eimer, T., Biedenkapp, A., Hutter, F., and Lindauer, M.\nSelf-paced context evaluation for contextual reinforce-\nment learning. In International Conference on Machine\nLearning , pp. 2948\u20132958. PMLR, 2021.\nFallah, A., Georgiev, K., Mokhtari, A., and Ozdaglar, A. On\nthe convergence theory of debiased model-agnostic meta-\nreinforcement learning. Advances in Neural Information\nProcessing Systems , 34:3096\u20133107, 2021.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Interna-\ntional conference on machine learning , pp. 1126\u20131135.\nPMLR, 2017.\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic\ngoal generation for reinforcement learning agents. In\nInternational conference on machine learning , pp. 1515\u2013\n1528. PMLR, 2018.\nHallak, A., Di Castro, D., and Mannor, S. Con-\ntextual markov decision processes. arXiv preprint\narXiv:1502.02259 , 2015.\nHarris, K., Anagnostides, I., Farina, G., Khodak, M., Wu,\nZ. S., and Sandholm, T. Meta-learning in games. arXiv\npreprint arXiv:2209.14110 , 2022.\nHe, S., Han, S., Su, S., Han, S., Zou, S., and Miao, F. Robust\nmulti-agent reinforcement learning with state uncertainty.\narXiv preprint arXiv:2307.16212 , 2023.\nJayawardana, V ., Freydt, B., Qu, A., Hickert, C., Yan, Z.,\nand Wu, C. Intersectionzoo: Eco-driving for benchmark-\ning multi-agent contextual reinforcement learning. arXiv\npreprint arXiv:2410.15221 , 2024.\nJiang, M., Grefenstette, E., and Rockt\u00e4schel, T. Prioritized\nlevel replay. In International Conference on Machine\nLearning , pp. 4940\u20134950. PMLR, 2021.\nJiang, Q., Qin, M., Shi, S., Sun, W., and Zheng, B. Multi-\nagent reinforcement learning for traffic signal control\nthrough universal communication method. arXiv preprint\narXiv:2204.12190 , 2022.\nKlink, P., D\u2019Eramo, C., Peters, J. R., and Pajarinen, J. Self-\npaced deep reinforcement learning. Advances in Neural\nInformation Processing Systems , 33:9216\u20139227, 2020.\nKurach, K., Raichuk, A., Sta \u00b4nczyk, P., Zaj \u02db ac, M., Bachem,\nO., Espeholt, L., Riquelme, C., Vincent, D., Michal-\nski, M., Bousquet, O., and Gelly, S. Google research\nfootball: A novel reinforcement learning environment.\nProceedings of the AAAI Conference on Artificial Intel-\nligence , 34(04):4501\u20134510, Apr. 2020. doi: 10.1609/\naaai.v34i04.5878. URL https://ojs.aaai.org/\nindex.php/AAAI/article/view/5878 .\n9\n\n--- Page 10 ---\ncMALC-D\nKwon, W., Li, Z., Zhuang, S., Sheng, Y ., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the ACM SIGOPS\n29th Symposium on Operating Systems Principles , 2023.\nLin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang,\nW.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq:\nActivation-aware weight quantization for on-device llm\ncompression and acceleration. Proceedings of Machine\nLearning and Systems , 6:87\u2013100, 2024.\nLiu, X., Hu, M., Peng, Y ., and Yang, Y . Multi-agent deep\nreinforcement learning for multi-echelon inventory man-\nagement. Production and Operations Management , pp.\n10591478241305863, 2022.\nMa, Y . J., Liang, W., Wang, G., Huang, D.-A., Bastani, O.,\nJayaraman, D., Zhu, Y ., Fan, L., and Anandkumar, A.\nEureka: Human-level reward design via coding large lan-\nguage models. arXiv preprint arXiv:2310.12931 , 2023.\nMa, Y . J., Liang, W., Wang, H.-J., Wang, S., Zhu, Y ., Fan,\nL., Bastani, O., and Jayaraman, D. Dreureka: Language\nmodel guided sim-to-real transfer, 2024. URL https:\n//arxiv.org/abs/2406.01967 .\nMao, W., Qiu, H., Wang, C., Franke, H., Kalbarczyk, Z.,\nIyer, R., and Basar, T. Multi-agent meta-reinforcement\nlearning: Sharper convergence rates with task sim-\nilarity. In Oh, A., Naumann, T., Globerson, A.,\nSaenko, K., Hardt, M., and Levine, S. (eds.), Ad-\nvances in Neural Information Processing Systems ,\nvolume 36, pp. 66556\u201366570. Curran Associates, Inc.,\n2023. URL https://proceedings.neurips.\ncc/paper_files/paper/2023/file/\nd1b1a091088904cbc7f7faa2b45c8f36-Paper-Conference.\npdf.\nMcClellan, J., Haghani, N., Winder, J., Huang, F., and\nTokekar, P. Boosting sample efficiency and generalization\nin multi-agent reinforcement learning via equivariance.\narXiv preprint arXiv:2410.02581 , 2024.\nMcClellan, J., Brothers, G., Huang, F., and Tokekar, P.\nPenguin: Partially equivariant graph neural networks for\nsample efficient marl. arXiv preprint arXiv:2503.15615 ,\n2025.\nMousa, M., van de Berg, D., Kotecha, N., del Rio Chanona,\nE. A., and Mowbray, M. An analysis of multi-agent\nreinforcement learning for decentralized inventory con-\ntrol systems. Computers & Chemical Engineering , 188:\n108783, 2024.\nNichol, A., Achiam, J., and Schulman, J. On\nfirst-order meta-learning algorithms. arXiv preprint\narXiv:1803.02999 , 2018.Papoudakis, G., Christianos, F., Sch\u00e4fer, L., and Albrecht,\nS. V . Benchmarking multi-agent deep reinforcement\nlearning algorithms in cooperative tasks. In Proceed-\nings of the Neural Information Processing Systems Track\non Datasets and Benchmarks (NeurIPS) , 2021. URL\nhttp://arxiv.org/abs/2006.07869 .\nParker-Holder, J., Jiang, M., Dennis, M., Samvelyan, M.,\nFoerster, J., Grefenstette, E., and Rockt\u00e4schel, T. Evolv-\ning curricula with regret-based environment design, 2023.\nURLhttps://arxiv.org/abs/2203.01302 .\nPortelas, R., Colas, C., Hofmann, K., and Oudeyer, P.-Y .\nTeacher algorithms for curriculum learning of deep rl in\ncontinuously parameterized environments. In Conference\non Robot Learning , pp. 835\u2013853. PMLR, 2020.\nSamvelyan, M., Rashid, T., de Witt, C. S., Farquhar, G.,\nNardelli, N., Rudner, T. G. J., Hung, C.-M., Torr, P. H. S.,\nFoerster, J., and Whiteson, S. The StarCraft Multi-Agent\nChallenge. CoRR , abs/1902.04043, 2019.\nSatheesh, A. and Powell, K. A constrained multi-agent\nreinforcement learning approach to autonomous traffic\nsignal control. arXiv preprint arXiv:2503.23626 , 2025.\nSatorras, V . G., Hoogeboom, E., and Welling, M. E(n) equiv-\nariant graph neural networks. In Meila, M. and Zhang,\nT. (eds.), Proceedings of the 38th International Confer-\nence on Machine Learning , volume 139 of Proceedings\nof Machine Learning Research , pp. 9323\u20139332. PMLR,\n18\u201324 Jul 2021. URL https://proceedings.mlr.\npress/v139/satorras21a.html .\nSkrynnik, A., Andreychuk, A., Nesterova, M., Yakovlev, K.,\nand Panov, A. Learn to follow: Decentralized lifelong\nmulti-agent pathfinding via planning and learning. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence , volume 38, pp. 17541\u201317549, 2024.\nSong, C. H., Wu, J., Washington, C., Sadler, B. M., Chao,\nW.-L., and Su, Y . Llm-planner: Few-shot grounded plan-\nning for embodied agents with large language models. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , October 2023.\nSukhbaatar, S., Kostrikov, I., Szlam, A., and Fergus, R. In-\ntrinsic motivation and automatic curricula via asymmetric\nself-play. 03 2017. doi: 10.48550/arXiv.1703.05407.\nTeam, Q. Qwen2.5: A party of foundation models, Septem-\nber 2024. URL https://qwenlm.github.io/\nblog/qwen2.5/ .\nTobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W.,\nand Abbeel, P. Domain randomization for transferring\ndeep neural networks from simulation to the real world.\nIn2017 IEEE/RSJ international conference on intelligent\nrobots and systems (IROS) , pp. 23\u201330. IEEE, 2017.\n10\n\n--- Page 11 ---\ncMALC-D\nTzeng, E., Devin, C., Hoffman, J., Finn, C., Abbeel, P.,\nLevine, S., Saenko, K., and Darrell, T. Adapting deep vi-\nsuomotor representations with weak pairwise constraints.\nInAlgorithmic Foundations of Robotics XII: Proceedings\nof the Twelfth Workshop on the Algorithmic Foundations\nof Robotics , pp. 688\u2013703. Springer, 2020.\nWei, H., Chen, C., Zheng, G., Wu, K., Gayah, V ., Xu, K.,\nand Li, Z. Presslight: Learning max pressure control\nto coordinate traffic signals in arterial network. In Pro-\nceedings of the 25th ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining , KDD\n\u201919, pp. 1290\u20131298, New York, NY , USA, 2019a. Associ-\nation for Computing Machinery. ISBN 9781450362016.\ndoi: 10.1145/3292500.3330949. URL https://doi.\norg/10.1145/3292500.3330949 .\nWei, H., Xu, N., Zhang, H., Zheng, G., Zang, X., Chen, C.,\nZhang, W., Zhu, Y ., Xu, K., and Li, Z. Colight: Learning\nnetwork-level cooperation for traffic signal control. In\nProceedings of the 28th ACM International Conference\non Information and Knowledge Management , CIKM \u201919,\npp. 1913\u20131922, New York, NY , USA, 2019b. Associa-\ntion for Computing Machinery. ISBN 9781450369763.\ndoi: 10.1145/3357384.3357902. URL https://doi.\norg/10.1145/3357384.3357902 .\nWen, L., Tseng, E. H., Peng, H., and Zhang, S. Dream\nto adapt: Meta reinforcement learning by latent context\nimagination and mdp imagination. IEEE Robotics and\nAutomation Letters , 2024.\nXu, Y ., Li, W., Vaezipoor, P., Sanner, S., and Khalil, E. B.\nLlms and the abstraction and reasoning corpus: Suc-\ncesses, failures, and the importance of object-based rep-\nresentations. arXiv preprint arXiv:2305.18354 , 2023.\nYang, Q., Xie, Z., Wei, H., Zhang, D., and Yang, Y .\nMallight: Influence-aware coordinated traffic signal con-\ntrol for traffic signal malfunctions, 2024. URL https:\n//arxiv.org/abs/2408.09768 .\nYao, S., Chen, H., Yang, J., and Narasimhan, K. Web-\nshop: Towards scalable real-world web interaction with\ngrounded language agents. Advances in Neural Informa-\ntion Processing Systems , 35:20744\u201320757, 2022.\nYu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y ., Bayen, A.,\nand Wu, Y . The surprising effectiveness of ppo in cooper-\native multi-agent games. Advances in neural information\nprocessing systems , 35:24611\u201324624, 2022.\nZala, A., Cho, J., Lin, H., Yoon, J., and Bansal, M. En-\nvgen: Generating and adapting environments via llms\nfor training embodied agents, 2024. URL https:\n//arxiv.org/abs/2403.12014 .Zhang, H., Feng, S., Liu, C., Ding, Y ., Zhu, Y ., Zhou, Z.,\nZhang, W., Yu, Y ., Jin, H., and Li, Z. Cityflow: A multi-\nagent reinforcement learning environment for large scale\ncity traffic scenario. In The world wide web conference ,\npp. 3620\u20133624, 2019.\nZhang, K., SUN, T., Tao, Y ., Genc, S., Mallya, S., and\nBasar, T. Robust multi-agent reinforcement learning\nwith model uncertainty. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.),\nAdvances in Neural Information Processing Systems ,\nvolume 33, pp. 10571\u201310583. Curran Associates, Inc.,\n2020. URL https://proceedings.neurips.\ncc/paper_files/paper/2020/file/\n774412967f19ea61d448977ad9749078-Paper.\npdf.\nZhang, M., Zhao, P., Luo, H., and Zhou, Z.-H. No-\nregret learning in time-varying zero-sum games. In\nChaudhuri, K., Jegelka, S., Song, L., Szepesvari,\nC., Niu, G., and Sabato, S. (eds.), Proceedings of\nthe 39th International Conference on Machine Learn-\ning, volume 162 of Proceedings of Machine Learn-\ning Research , pp. 26772\u201326808. PMLR, 17\u201323 Jul\n2022. URL https://proceedings.mlr.press/\nv162/zhang22an.html .\nZhang, S., Shen, L., and Han, L. Learning meta represen-\ntations for agents in multi-agent reinforcement learning.\narXiv preprint arXiv:2108.12988 , 2021.\nZhang, X.-Y ., Liu, Y ., Arcaini, P., Jiang, M., and Zheng, Z.\nMet-mapf: A metamorphic testing approach for multi-\nagent path finding algorithms. ACM Transactions on Soft-\nware Engineering and Methodology , 33(8):1\u201337, 2024.\nZhao, W., Queralta, J. P., and Westerlund, T. Sim-to-real\ntransfer in deep reinforcement learning for robotics: a\nsurvey. In 2020 IEEE Symposium Series on Compu-\ntational Intelligence (SSCI) , pp. 737\u2013744, 2020. doi:\n10.1109/SSCI47803.2020.9308468.\n11\n\n--- Page 12 ---\ncMALC-D\nA. Additional Experimental Details\nA.1. Dataset Details\nWe use the Cityflow environment with three publicly available datasets derived from real-world data. We use the Jinan\n(1\u00d73)dataset from PressLight (Wei et al., 2019a) and the Jinan (3\u00d74)and Hangzhou (4\u00d74)datasets from CoLight (Wei\net al., 2019b). Each dataset contains a roadnet.json file that gives the topology of the road network and a flow.json\nfile that gives the vehicle routes and flow patterns. A more detailed list of dataset statistics can be found in Table 2.\nA.2. Environment Details\nIn our experiments, each agent corresponds to a traffic light controlling an intersection. Therefore, the Jinan environments\nhave 3 and 12 agents, respectively, while the Hangzhou environment has 16 agents. Each traffic light agent can choose from\n8 possible actions, where each action represents a specific signal phase. A phase refers to a predefined combination of green\nand red signals assigned to incoming lanes, typically allowing certain directions of traffic (e.g., straight-through, left-turn)\nwhile restricting others. Figure 2 illustrates each phase. The observation for each intersection is composed of information\nabout all ingoing and outgoing cars in each lane, such as their position, velocity, and lane number. Each agent has partial\nobservability and cannot use information that is outside the local observation to make decisions. At time t, the global state\nstis the concatenation of all local observations. The global reward is defined as \u03bbfTf+\u03bbwTw, where Tfis the total time\nvehicles are moving, Twis the total time vehicles are waiting in traffic, and \u03bbf, \u03bbware hyperparameters. In practice, we set\n\u03bbf= 0.033and\u03bbw= 0.\nWe evaluate our traffic control policies using several real-world metrics that reflect their effectiveness. Wait time measures\nthe total duration that vehicles remain stationary (i.e., with zero speed) during their journeys. Delay time captures the\nadditional time each vehicle takes to reach its destination compared to the ideal travel time in the absence of any traffic\nsignals or congestion. Finally, we calculate the throughput, which is the total number of vehicles that complete their routes\nbefore the end of the episode.\nDataset # Intersections Mean Std Max Min # Lanes Time Steps\nHangzhou 16 526.63 86.70 676 256 3 3600\nJinan (3\u00d74) 12 250.70 38.21 335 208 3 3600\nJinan (1\u00d73) 3 278.23 18.47 322 229 3 3600\nTable 2. Data statistics of real-world traffic datasets. Arrival rates are reported vehicles per 300 seconds.\nFigure 2. Illustration of all 8 phases for each intersection. Phases are designed so that only two lanes can be active at any time.\n12\n\n--- Page 13 ---\ncMALC-D\nA.3. Context Parameterization Details\nParameter Description Range\nlength Length of each car 1\u201310m\nwidth Width of each car 1\u20135m\nmaxPosAcc Max acceleration when speeding up 0.5\u20135m/s2\nmaxNegAcc Max deceleration when braking 0.5\u20135m/s2\nusualPosAcc Default acceleration when speeding up 1\u20135m/s2\nusualNegAcc Default deceleration when braking 1\u20135m/s2\nminGap Minimum gap between cars 1\u201310m\nmaxSpeed Maximum speed a car can travel 3\u201315m/s\nheadwayTime Desired time to reach the vehicle in front 1\u20135s (int)\nTable 3. Context parameters used for curriculum learning and their specified ranges.\nA.4. Hyperparameters\nThe MAPPO algorithm is adapted from the ePYMARL library (Papoudakis et al., 2021). We provide a full list of algorithmic\nenvironment hyperparameters in Table 4. We also use an Adam optimizer with a learning rate of 0.003to train the cost\nestimator. We also list the hyperparameters of cMALC-D and the parameters used for the LLM in Tables 5 and 6 respectively.\nTable 4. Hyperparameter for MAPPO\nMAPPO Hyperparameters\nEps Clip 0.2\nEpsilon Anneal Time 180000\nLearning Rate (lr) 0.0003\nHidden Dim 128\nMini Epochs 4\nEntropy Coef 0.001\nTarget Update Interval 0.01\nBatch Size 1\nBuffer Size 10\nTable 5. cMALC-D Hyperparameters\nHyperparameter\nA MAPPO\nM Qwen2.5-7B-Instruct-AWQ\n\u03b1 0.5\nw 3\n\u03b4 0.1\nk 3\nTable 6. LLM Hyperparameters\nHyperparameter\nTemperature 0.7\nTop-p 0.9\nMax New Tokens 400\n13\n\n--- Page 14 ---\ncMALC-D\nB. Further Results\nFigure 3. Individual feature values over time for cMALC-D for JN 1\u00d73.\nFigure 4. Individual feature values over time for cMALC-D for HZ.\nFigure 5. Individual feature values over time for cMALC-D for JN 3\u00d74.\nFigure 6. Correlation matrix for cMALC-D generated curriculum on the JN 1\u00d73environment.\n14\n\n--- Page 15 ---\ncMALC-D\nFigure 7. Correlation matrix for cMALC-D generated curriculum on the HZ environment.\nFigure 8. Correlation matrix for cMALC-D generated curriculum on the JN environment.\nB.1. LLM Prompts\nTable 7: The following table contains the prompts used for the LLM, both initially and after receiving data from the\nenvironments.\n15\n\n--- Page 16 ---\ncMALC-D\nThis is the initial prompt given to the LLM.\nSystem: You are a curriculum designer for traffic simulations. Your goal is to generate a curriculum for training an agent to optimize traffic flow. This cur-\nriculum needs to test the agent\u2019s ability to handle various traffic conditions. Generate one plausible set of car parameters within the given bounds:\n- length: (1.0-10.0)\n- width: (1.0-5.0)\n- maxPosAcc: (0.5-5.0)\n- maxNegAcc: (0.5-5.0)\n- usualPosAcc: (1.0-5.0)\n- usualNegAcc: (1.0-5.0)\n- minGap: (1.0-10.0)\n- maxSpeed: (3.0-15.0)\n- headwayTime: (1-5, integer)\nOutput a single JSON object of parameter values.\nLLM: ...\nThis is the prompt given to the LLM at all later timesteps, after having results from previous contexts\nSystem: You are a curriculum designer for traffic light simulation. Your goal is to generate a curriculum for training an agent to optimize traffic flow. This\ncurriculum needs to test the agent\u2019s ability to handle various traffic conditions.\nUse the past trial data to propose the next car configuration for training.\nAnalyze these past car parameter trials and determine how to generate the next task:\n[results given in json format here]\nCar Performance Assessment:\n1. What parameter combinations were successful?\n2. What weaknesses should be addressed?\n3. What logical evolutions can we make?\n4. Suggest specific values or parameter patterns to evolve the curriculum.\nFormat your response as:\n- 1-2 sentences summarizing key insights\n- Then \"NEXT TASK SUGGESTION:\" followed by a JSON object of new car parameters satisfying the bounds:\n- length: (1.0-10.0)\n- width: (1.0-5.0)\n- maxPosAcc: (0.5-5.0)\n- maxNegAcc: (0.5-5.0)\n- usualPosAcc: (1.0-5.0)\n- usualNegAcc: (1.0-5.0)\n- minGap: (1.0-10.0)\n- maxSpeed: (3.0-15.0)\n- headwayTime: (1-5, integer)\nPlease generate your insights and new parameters.\nLLM: ...\n16",
  "project_dir": "artifacts/projects/enhanced_cs.MA_2508.20818v1_cMALC_D_Contextual_Multi_Agent_LLM_Guided_Curricu",
  "communication_dir": "artifacts/projects/enhanced_cs.MA_2508.20818v1_cMALC_D_Contextual_Multi_Agent_LLM_Guided_Curricu/.agent_comm",
  "assigned_at": "2025-08-29T20:53:29.168766",
  "status": "assigned"
}